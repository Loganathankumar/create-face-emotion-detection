{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Loganathankumar/create-face-emotion-detection/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtOvXHc87cjQ",
        "outputId": "5f96b74c-59c3-464c-b4c8-bcfedabfbd24"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4048571588.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# Mount Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# ===================== CONFIGURATION =====================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Advanced Facial Expression Recognition using RAVDESS Dataset\n",
        "PART 1: Data Loading and Preprocessing\n",
        "Processes VIDEO files to extract facial expressions\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ===================== CONFIGURATION =====================\n",
        "class Config:\n",
        "    DATASET_PATH = '/content/drive/MyDrive/RAVDESS DATASET'\n",
        "    IMG_SIZE = (128, 128)\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 100\n",
        "    LEARNING_RATE = 0.001\n",
        "    RANDOM_STATE = 42\n",
        "    FRAMES_PER_VIDEO = 5  # Extract N frames from each video\n",
        "\n",
        "    # Emotion mapping for RAVDESS\n",
        "    EMOTIONS = {\n",
        "        '01': 'neutral',\n",
        "        '02': 'calm',\n",
        "        '03': 'happy',\n",
        "        '04': 'sad',\n",
        "        '05': 'angry',\n",
        "        '06': 'fearful',\n",
        "        '07': 'disgust',\n",
        "        '08': 'surprised'\n",
        "    }\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "np.random.seed(config.RANDOM_STATE)\n",
        "tf.random.set_seed(config.RANDOM_STATE)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ===================== FACE DETECTION =====================\n",
        "class FaceDetector:\n",
        "    def __init__(self):\n",
        "        # Load Haar Cascade for face detection\n",
        "        self.face_cascade = cv2.CascadeClassifier(\n",
        "            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
        "        )\n",
        "        print(\"✓ Face detector initialized\")\n",
        "\n",
        "    def detect_face(self, frame):\n",
        "        \"\"\"Detect and extract face from frame\"\"\"\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        faces = self.face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "        if len(faces) > 0:\n",
        "            # Get largest face\n",
        "            (x, y, w, h) = max(faces, key=lambda rect: rect[2] * rect[3])\n",
        "            face = frame[y:y+h, x:x+w]\n",
        "            return face\n",
        "        return None\n",
        "\n",
        "# ===================== DATA LOADING & PREPROCESSING =====================\n",
        "class RAVDESSDataLoader:\n",
        "    def __init__(self, dataset_path):\n",
        "        self.dataset_path = dataset_path\n",
        "        self.data = []\n",
        "        self.face_detector = FaceDetector()\n",
        "\n",
        "    def extract_frames_from_video(self, video_path):\n",
        "        \"\"\"Extract frames from video and detect faces\"\"\"\n",
        "        try:\n",
        "            cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "            if not cap.isOpened():\n",
        "                return []\n",
        "\n",
        "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "            if total_frames == 0:\n",
        "                cap.release()\n",
        "                return []\n",
        "\n",
        "            # Calculate frame indices to extract (evenly spaced)\n",
        "            frame_indices = np.linspace(0, total_frames - 1,\n",
        "                                       config.FRAMES_PER_VIDEO, dtype=int)\n",
        "\n",
        "            faces = []\n",
        "            for frame_idx in frame_indices:\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
        "                ret, frame = cap.read()\n",
        "\n",
        "                if ret:\n",
        "                    # Detect face in frame\n",
        "                    face = self.face_detector.detect_face(frame)\n",
        "\n",
        "                    if face is not None:\n",
        "                        # Resize face to target size\n",
        "                        face_resized = cv2.resize(face, config.IMG_SIZE)\n",
        "                        # Convert BGR to RGB\n",
        "                        face_rgb = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)\n",
        "                        faces.append(face_rgb)\n",
        "\n",
        "            cap.release()\n",
        "            return faces\n",
        "\n",
        "        except Exception as e:\n",
        "            return []\n",
        "\n",
        "    def parse_filename(self, filename):\n",
        "        \"\"\"Parse RAVDESS filename to extract emotion label\"\"\"\n",
        "        parts = filename.split('-')\n",
        "        if len(parts) >= 3:\n",
        "            emotion_code = parts[2]\n",
        "            return config.EMOTIONS.get(emotion_code, None)\n",
        "        return None\n",
        "\n",
        "    def load_dataset(self):\n",
        "        \"\"\"Load all video files and extract faces\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"LOADING RAVDESS DATASET\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"Dataset path: {self.dataset_path}\")\n",
        "\n",
        "        if not os.path.exists(self.dataset_path):\n",
        "            print(f\"ERROR: Dataset path does not exist: {self.dataset_path}\")\n",
        "            return []\n",
        "\n",
        "        # Get all actor folders\n",
        "        all_items = os.listdir(self.dataset_path)\n",
        "        actor_folders = [f for f in all_items if f.startswith('Actor_')]\n",
        "\n",
        "        print(f\"Found {len(actor_folders)} actor folders\")\n",
        "\n",
        "        if len(actor_folders) == 0:\n",
        "            print(\"ERROR: No actor folders found!\")\n",
        "            return []\n",
        "\n",
        "        video_count = 0\n",
        "        face_count = 0\n",
        "\n",
        "        for actor_folder in tqdm(actor_folders, desc=\"Processing actors\"):\n",
        "            actor_path = os.path.join(self.dataset_path, actor_folder)\n",
        "\n",
        "            if not os.path.isdir(actor_path):\n",
        "                continue\n",
        "\n",
        "            # Get all video files\n",
        "            all_files = os.listdir(actor_path)\n",
        "            video_files = [f for f in all_files\n",
        "                          if f.endswith(('.mp4', '.avi', '.mov', '.MP4', '.AVI', '.MOV'))]\n",
        "\n",
        "            for video_file in video_files:\n",
        "                emotion = self.parse_filename(video_file)\n",
        "\n",
        "                if emotion is None:\n",
        "                    continue\n",
        "\n",
        "                video_path = os.path.join(actor_path, video_file)\n",
        "                faces = self.extract_frames_from_video(video_path)\n",
        "\n",
        "                video_count += 1\n",
        "\n",
        "                # Add each face as a separate sample\n",
        "                for face in faces:\n",
        "                    self.data.append({\n",
        "                        'features': face,\n",
        "                        'emotion': emotion,\n",
        "                        'actor': actor_folder,\n",
        "                        'filename': video_file\n",
        "                    })\n",
        "                    face_count += 1\n",
        "\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Processing complete:\")\n",
        "        print(f\"  Videos processed: {video_count}\")\n",
        "        print(f\"  Face frames extracted: {face_count}\")\n",
        "        print(f\"  Avg faces/video: {face_count/video_count if video_count > 0 else 0:.1f}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        return self.data\n",
        "\n",
        "    def prepare_data(self):\n",
        "        \"\"\"Prepare data for training\"\"\"\n",
        "        if not self.data:\n",
        "            self.load_dataset()\n",
        "\n",
        "        if len(self.data) == 0:\n",
        "            raise ValueError(\"No data was loaded from the dataset\")\n",
        "\n",
        "        # Convert to arrays\n",
        "        X = np.array([item['features'] for item in self.data])\n",
        "        y = np.array([item['emotion'] for item in self.data])\n",
        "\n",
        "        # Normalize features\n",
        "        X = X / 255.0\n",
        "\n",
        "        # Encode labels\n",
        "        le = LabelEncoder()\n",
        "        y_encoded = le.fit_transform(y)\n",
        "        y_categorical = keras.utils.to_categorical(y_encoded)\n",
        "\n",
        "        print(f\"{'='*80}\")\n",
        "        print(\"DATASET SUMMARY\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Data shape: {X.shape}\")\n",
        "        print(f\"Labels shape: {y_categorical.shape}\")\n",
        "        print(f\"Number of classes: {len(le.classes_)}\")\n",
        "        print(f\"Classes: {list(le.classes_)}\")\n",
        "\n",
        "        # Class distribution\n",
        "        unique, counts = np.unique(y, return_counts=True)\n",
        "        print(f\"\\n{'Class Distribution':^40}\")\n",
        "        print(f\"{'-'*40}\")\n",
        "        for emotion, count in zip(unique, counts):\n",
        "            print(f\"  {emotion:12} : {count:4d} samples ({count/len(y)*100:5.1f}%)\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "\n",
        "        return X, y_categorical, le\n",
        "\n",
        "# ===================== DATA SPLITTING =====================\n",
        "def split_data(X, y, test_size=0.2, val_size=0.15):\n",
        "    \"\"\"Split data with stratification\"\"\"\n",
        "    y_labels = np.argmax(y, axis=1)\n",
        "\n",
        "    # First split: train+val and test\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        X, y, test_size=test_size,\n",
        "        stratify=y_labels, random_state=config.RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    # Second split: train and val\n",
        "    y_temp_labels = np.argmax(y_temp, axis=1)\n",
        "    val_size_adjusted = val_size / (1 - test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_size_adjusted,\n",
        "        stratify=y_temp_labels, random_state=config.RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    print(f\"{'='*80}\")\n",
        "    print(\"DATA SPLIT\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Training set:   {X_train.shape[0]:5d} samples ({X_train.shape[0]/X.shape[0]*100:.1f}%)\")\n",
        "    print(f\"Validation set: {X_val.shape[0]:5d} samples ({X_val.shape[0]/X.shape[0]*100:.1f}%)\")\n",
        "    print(f\"Test set:       {X_test.shape[0]:5d} samples ({X_test.shape[0]/X.shape[0]*100:.1f}%)\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# ===================== EXECUTE DATA LOADING =====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING DATA PREPARATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load and prepare data\n",
        "loader = RAVDESSDataLoader(config.DATASET_PATH)\n",
        "X, y, label_encoder = loader.prepare_data()\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
        "\n",
        "# Save processed data for later use\n",
        "print(\"Saving preprocessed data...\")\n",
        "np.save('X_train.npy', X_train)\n",
        "np.save('X_val.npy', X_val)\n",
        "np.save('X_test.npy', X_test)\n",
        "np.save('y_train.npy', y_train)\n",
        "np.save('y_val.npy', y_val)\n",
        "np.save('y_test.npy', y_test)\n",
        "\n",
        "# Save label encoder classes\n",
        "import pickle\n",
        "with open('label_encoder.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "print(\"✅ Data preparation complete!\")\n",
        "print(f\"{'='*80}\\n\")\n",
        "print(\"Run PART 2 to train models!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Advanced Facial Expression Recognition using RAVDESS Dataset\n",
        "PART 2: Deep Learning Model Architectures\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.applications import (\n",
        "    VGG16, ResNet50, InceptionV3, MobileNetV2,\n",
        "    EfficientNetB0, DenseNet121, Xception\n",
        ")\n",
        "import pickle\n",
        "\n",
        "# Load preprocessed data\n",
        "print(\"=\"*80)\n",
        "print(\"LOADING PREPROCESSED DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "X_train = np.load('X_train.npy')\n",
        "X_val = np.load('X_val.npy')\n",
        "X_test = np.load('X_test.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "y_val = np.load('y_val.npy')\n",
        "y_test = np.load('y_test.npy')\n",
        "\n",
        "with open('label_encoder.pkl', 'rb') as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "input_shape = X_train.shape[1:]\n",
        "num_classes = y_train.shape[1]\n",
        "\n",
        "print(f\"✓ Data loaded successfully\")\n",
        "print(f\"  Input shape: {input_shape}\")\n",
        "print(f\"  Number of classes: {num_classes}\")\n",
        "print(f\"  Training samples: {X_train.shape[0]}\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ===================== MODEL 1: CUSTOM CNN =====================\n",
        "def create_custom_cnn(input_shape, num_classes):\n",
        "    \"\"\"Custom CNN architecture optimized for facial expressions\"\"\"\n",
        "    print(\"\\n[MODEL 1] Building Custom CNN...\")\n",
        "\n",
        "    model = models.Sequential([\n",
        "        # Block 1\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Block 2\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Block 3\n",
        "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Block 4\n",
        "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        # Dense layers\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='Custom_CNN')\n",
        "\n",
        "    print(f\"✓ Custom CNN created - Total params: {model.count_params():,}\")\n",
        "    return model\n",
        "\n",
        "# ===================== MODEL 2: ATTENTION CNN =====================\n",
        "def create_attention_cnn(input_shape, num_classes):\n",
        "    \"\"\"CNN with Spatial Attention Mechanism\"\"\"\n",
        "    print(\"\\n[MODEL 2] Building Attention-based CNN...\")\n",
        "\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Block 1\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Block 2\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Block 3 with Attention\n",
        "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Spatial Attention\n",
        "    attention = layers.Conv2D(1, (1, 1), activation='sigmoid', padding='same', name='attention_map')(x)\n",
        "    x = layers.Multiply()([x, attention])\n",
        "\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Block 4\n",
        "    x = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Dense layers\n",
        "    x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name='Attention_CNN')\n",
        "    print(f\"✓ Attention CNN created - Total params: {model.count_params():,}\")\n",
        "    return model\n",
        "\n",
        "# ===================== MODEL 3: RESIDUAL CNN =====================\n",
        "def create_residual_cnn(input_shape, num_classes):\n",
        "    \"\"\"Custom CNN with Residual Connections\"\"\"\n",
        "    print(\"\\n[MODEL 3] Building Residual CNN...\")\n",
        "\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Initial convolution\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Residual Block 1\n",
        "    shortcut = x\n",
        "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(64, (3, 3), padding='same')(x)\n",
        "    x = layers.Add()([x, shortcut])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Residual Block 2\n",
        "    x = layers.Conv2D(128, (1, 1), padding='same')(x)  # Match dimensions\n",
        "    shortcut = x\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(128, (3, 3), padding='same')(x)\n",
        "    x = layers.Add()([x, shortcut])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Residual Block 3\n",
        "    x = layers.Conv2D(256, (1, 1), padding='same')(x)\n",
        "    shortcut = x\n",
        "    x = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Conv2D(256, (3, 3), padding='same')(x)\n",
        "    x = layers.Add()([x, shortcut])\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "\n",
        "    # Global pooling and dense\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs, name='Residual_CNN')\n",
        "    print(f\"✓ Residual CNN created - Total params: {model.count_params():,}\")\n",
        "    return model\n",
        "\n",
        "# ===================== MODEL 4: VGG16 TRANSFER LEARNING =====================\n",
        "def create_vgg16_transfer(input_shape, num_classes):\n",
        "    \"\"\"VGG16 with Transfer Learning\"\"\"\n",
        "    print(\"\\n[MODEL 4] Building VGG16 Transfer Learning Model...\")\n",
        "\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False  # Freeze base layers\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='VGG16_Transfer')\n",
        "\n",
        "    print(f\"✓ VGG16 Transfer created - Total params: {model.count_params():,}\")\n",
        "    print(f\"  Trainable params: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n",
        "    return model\n",
        "\n",
        "# ===================== MODEL 5: RESNET50 TRANSFER LEARNING =====================\n",
        "def create_resnet50_transfer(input_shape, num_classes):\n",
        "    \"\"\"ResNet50 with Transfer Learning\"\"\"\n",
        "    print(\"\\n[MODEL 5] Building ResNet50 Transfer Learning Model...\")\n",
        "\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='ResNet50_Transfer')\n",
        "\n",
        "    print(f\"✓ ResNet50 Transfer created - Total params: {model.count_params():,}\")\n",
        "    print(f\"  Trainable params: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n",
        "    return model\n",
        "\n",
        "# ===================== MODEL 6: MOBILENETV2 TRANSFER LEARNING =====================\n",
        "def create_mobilenet_transfer(input_shape, num_classes):\n",
        "    \"\"\"MobileNetV2 with Transfer Learning - Lightweight & Fast\"\"\"\n",
        "    print(\"\\n[MODEL 6] Building MobileNetV2 Transfer Learning Model...\")\n",
        "\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='MobileNetV2_Transfer')\n",
        "\n",
        "    print(f\"✓ MobileNetV2 Transfer created - Total params: {model.count_params():,}\")\n",
        "    print(f\"  Trainable params: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n",
        "    return model\n",
        "\n",
        "# ===================== MODEL 7: EFFICIENTNET TRANSFER LEARNING =====================\n",
        "def create_efficientnet_transfer(input_shape, num_classes):\n",
        "    \"\"\"EfficientNetB0 with Transfer Learning\"\"\"\n",
        "    print(\"\\n[MODEL 7] Building EfficientNetB0 Transfer Learning Model...\")\n",
        "\n",
        "    base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='EfficientNetB0_Transfer')\n",
        "\n",
        "    print(f\"✓ EfficientNetB0 Transfer created - Total params: {model.count_params():,}\")\n",
        "    print(f\"  Trainable params: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n",
        "    return model\n",
        "\n",
        "# ===================== MODEL 8: DENSENET TRANSFER LEARNING =====================\n",
        "def create_densenet_transfer(input_shape, num_classes):\n",
        "    \"\"\"DenseNet121 with Transfer Learning\"\"\"\n",
        "    print(\"\\n[MODEL 8] Building DenseNet121 Transfer Learning Model...\")\n",
        "\n",
        "    base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False\n",
        "\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ], name='DenseNet121_Transfer')\n",
        "\n",
        "    print(f\"✓ DenseNet121 Transfer created - Total params: {model.count_params():,}\")\n",
        "    print(f\"  Trainable params: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n",
        "    return model\n",
        "\n",
        "# ===================== CREATE ALL MODELS =====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CREATING ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "models_dict = {\n",
        "    'Custom_CNN': create_custom_cnn(input_shape, num_classes),\n",
        "    'Attention_CNN': create_attention_cnn(input_shape, num_classes),\n",
        "    'Residual_CNN': create_residual_cnn(input_shape, num_classes),\n",
        "    'VGG16_Transfer': create_vgg16_transfer(input_shape, num_classes),\n",
        "    'ResNet50_Transfer': create_resnet50_transfer(input_shape, num_classes),\n",
        "    'MobileNetV2_Transfer': create_mobilenet_transfer(input_shape, num_classes),\n",
        "    'EfficientNetB0_Transfer': create_efficientnet_transfer(input_shape, num_classes),\n",
        "    'DenseNet121_Transfer': create_densenet_transfer(input_shape, num_classes),\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"✅ ALL {len(models_dict)} MODELS CREATED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Save models dictionary\n",
        "import pickle\n",
        "with open('models_dict.pkl', 'wb') as f:\n",
        "    pickle.dump(models_dict, f)\n",
        "\n",
        "print(\"\\n✓ Models saved to 'models_dict.pkl'\")\n",
        "print(\"\\nRun PART 3 to train all models!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "rIoAcxB8GNM1",
        "outputId": "f4e41a08-600a-4e32-c7cb-0e9a481b04f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "LOADING PREPROCESSED DATA\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'X_train.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2758644539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X_val.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'X_test.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'X_train.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Advanced Facial Expression Recognition using RAVDESS Dataset\n",
        "PART 3: Training Pipeline with Anti-Overfitting Techniques\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import pickle\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load data and models\n",
        "print(\"=\"*80)\n",
        "print(\"LOADING DATA AND MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "X_train = np.load('X_train.npy')\n",
        "X_val = np.load('X_val.npy')\n",
        "X_test = np.load('X_test.npy')\n",
        "y_train = np.load('y_train.npy')\n",
        "y_val = np.load('y_val.npy')\n",
        "y_test = np.load('y_test.npy')\n",
        "\n",
        "with open('label_encoder.pkl', 'rb') as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "with open('models_dict.pkl', 'rb') as f:\n",
        "    models_dict = pickle.load(f)\n",
        "\n",
        "print(f\"✓ Data loaded\")\n",
        "print(f\"✓ {len(models_dict)} models loaded\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ===================== TRAINING CONFIGURATION =====================\n",
        "class TrainingConfig:\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 100\n",
        "    LEARNING_RATE = 0.001\n",
        "\n",
        "config = TrainingConfig()\n",
        "\n",
        "# ===================== DATA AUGMENTATION =====================\n",
        "def get_data_augmentation():\n",
        "    \"\"\"Data augmentation to prevent overfitting\"\"\"\n",
        "    return ImageDataGenerator(\n",
        "        rotation_range=15,\n",
        "        width_shift_range=0.15,\n",
        "        height_shift_range=0.15,\n",
        "        horizontal_flip=True,\n",
        "        zoom_range=0.15,\n",
        "        shear_range=0.1,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "datagen = get_data_augmentation()\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# ===================== CALLBACKS =====================\n",
        "def get_callbacks(model_name):\n",
        "    \"\"\"Get training callbacks for anti-overfitting\"\"\"\n",
        "    return [\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=20,\n",
        "            restore_best_weights=True,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=10,\n",
        "            min_lr=1e-7,\n",
        "            verbose=1\n",
        "        ),\n",
        "        ModelCheckpoint(\n",
        "            f'best_{model_name}.h5',\n",
        "            monitor='val_accuracy',\n",
        "            save_best_only=True,\n",
        "            verbose=0\n",
        "        )\n",
        "    ]\n",
        "\n",
        "# ===================== CLASS WEIGHTS =====================\n",
        "def calculate_class_weights(y):\n",
        "    \"\"\"Calculate class weights for imbalanced data\"\"\"\n",
        "    y_integers = np.argmax(y, axis=1)\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight='balanced',\n",
        "        classes=np.unique(y_integers),\n",
        "        y=y_integers\n",
        "    )\n",
        "    return dict(enumerate(class_weights))\n",
        "\n",
        "class_weights = calculate_class_weights(y_train)\n",
        "print(\"Class weights calculated to handle imbalanced data\")\n",
        "print(f\"Class weights: {class_weights}\\n\")\n",
        "\n",
        "# ===================== TRAINING FUNCTION =====================\n",
        "def train_model(model, model_name, X_train, X_val, y_train, y_val):\n",
        "    \"\"\"Train a single model with all anti-overfitting techniques\"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TRAINING: {model_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.Adam(learning_rate=config.LEARNING_RATE),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "\n",
        "    # Get callbacks\n",
        "    callbacks = get_callbacks(model_name)\n",
        "\n",
        "    print(f\"Starting training with:\")\n",
        "    print(f\"  - Data augmentation\")\n",
        "    print(f\"  - Early stopping (patience=20)\")\n",
        "    print(f\"  - Learning rate reduction (patience=10)\")\n",
        "    print(f\"  - Class weight balancing\")\n",
        "    print(f\"  - Batch size: {config.BATCH_SIZE}\")\n",
        "    print(f\"  - Max epochs: {config.EPOCHS}\")\n",
        "    print()\n",
        "\n",
        "    # Train with data augmentation\n",
        "    history = model.fit(\n",
        "        datagen.flow(X_train, y_train, batch_size=config.BATCH_SIZE),\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=config.EPOCHS,\n",
        "        callbacks=callbacks,\n",
        "        class_weight=class_weights,\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    print(f\"\\n✓ Training completed for {model_name}\")\n",
        "    print(f\"  Best val_accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
        "    print(f\"  Epochs trained: {len(history.history['loss'])}\")\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# ===================== PLOTTING FUNCTION =====================\n",
        "def plot_training_history(history, model_name):\n",
        "    \"\"\"Plot training history\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "    # Accuracy\n",
        "    axes[0, 0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
        "    axes[0, 0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
        "    axes[0, 0].set_title(f'{model_name} - Accuracy', fontsize=14, fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Accuracy')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Loss\n",
        "    axes[0, 1].plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
        "    axes[0, 1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
        "    axes[0, 1].set_title(f'{model_name} - Loss', fontsize=14, fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('Loss')\n",
        "    axes[0, 1].legend()\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # AUC\n",
        "    axes[1, 0].plot(history.history['auc'], label='Train AUC', linewidth=2)\n",
        "    axes[1, 0].plot(history.history['val_auc'], label='Val AUC', linewidth=2)\n",
        "    axes[1, 0].set_title(f'{model_name} - AUC', fontsize=14, fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('AUC')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Overfitting detection\n",
        "    train_val_gap = np.array(history.history['accuracy']) - np.array(history.history['val_accuracy'])\n",
        "    axes[1, 1].plot(train_val_gap, label='Train-Val Gap', linewidth=2, color='red')\n",
        "    axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
        "    axes[1, 1].set_title(f'{model_name} - Overfitting Detection', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Epoch')\n",
        "    axes[1, 1].set_ylabel('Accuracy Gap')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name}_training_history.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"✓ Training plot saved: {model_name}_training_history.png\")\n",
        "\n",
        "# ===================== TRAIN ALL MODELS =====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STARTING TRAINING FOR ALL MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "training_results = {}\n",
        "\n",
        "for model_name, model in models_dict.items():\n",
        "    try:\n",
        "        # Train model\n",
        "        trained_model, history = train_model(\n",
        "            model, model_name, X_train, X_val, y_train, y_val\n",
        "        )\n",
        "\n",
        "        # Plot training history\n",
        "        plot_training_history(history, model_name)\n",
        "\n",
        "        # Store results\n",
        "        training_results[model_name] = {\n",
        "            'model': trained_model,\n",
        "            'history': history.history\n",
        "        }\n",
        "\n",
        "        # Save model\n",
        "        trained_model.save(f'{model_name}_final.h5')\n",
        "        print(f\"✓ Model saved: {model_name}_final.h5\\n\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error training {model_name}: {str(e)}\\n\")\n",
        "        continue\n",
        "\n",
        "# Save training results\n",
        "with open('training_results.pkl', 'wb') as f:\n",
        "    pickle.dump(training_results, f)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(f\"✅ TRAINING COMPLETED FOR {len(training_results)} MODELS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nRun PART 4 to evaluate all models!\")"
      ],
      "metadata": {
        "id": "Vhz3A3TM9k3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Advanced Facial Expression Recognition using RAVDESS Dataset\n",
        "PART 4: Model Evaluation and Comparison\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, cohen_kappa_score\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# Load data and results\n",
        "print(\"=\"*80)\n",
        "print(\"LOADING DATA AND TRAINED MODELS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "X_test = np.load('X_test.npy')\n",
        "y_test = np.load('y_test.npy')\n",
        "\n",
        "with open('label_encoder.pkl', 'rb') as f:\n",
        "    label_encoder = pickle.load(f)\n",
        "\n",
        "with open('training_results.pkl', 'rb') as f:\n",
        "    training_results = pickle.load(f)\n",
        "\n",
        "print(f\"✓ Test data loaded: {X_test.shape[0]} samples\")\n",
        "print(f\"✓ {len(training_results)} trained models loaded\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ===================== EVALUATION FUNCTION =====================\n",
        "def evaluate_model(model, model_name, X_test, y_test):\n",
        "    \"\"\"Comprehensive model evaluation\"\"\"\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "\n",
        "    # Predictions\n",
        "    y_pred_prob = model.predict(X_test, verbose=0)\n",
        "    y_pred_classes = np.argmax(y_pred_prob, axis=1)\n",
        "    y_test_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
        "\n",
        "    # Per-class metrics\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(\n",
        "        y_test_classes, y_pred_classes, average=None\n",
        "    )\n",
        "\n",
        "    # Overall metrics\n",
        "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
        "        y_test_classes, y_pred_classes, average='macro'\n",
        "    )\n",
        "\n",
        "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
        "        y_test_classes, y_pred_classes, average='weighted'\n",
        "    )\n",
        "\n",
        "    # Cohen's Kappa\n",
        "    kappa = cohen_kappa_score(y_test_classes, y_pred_classes)\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test_classes, y_pred_classes)\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(\n",
        "        y_test_classes, y_pred_classes,\n",
        "        target_names=label_encoder.classes_,\n",
        "        output_dict=True\n",
        "    )\n",
        "\n",
        "    results = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'precision_weighted': precision_weighted,\n",
        "        'recall_weighted': recall_weighted,\n",
        "        'f1_weighted': f1_weighted,\n",
        "        'kappa': kappa,\n",
        "        'confusion_matrix': cm,\n",
        "        'report': report,\n",
        "        'predictions': y_pred_classes,\n",
        "        'true_labels': y_test_classes,\n",
        "        'pred_probabilities': y_pred_prob\n",
        "    }\n",
        "\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  F1-Score (Macro): {f1_macro:.4f}\")\n",
        "    print(f\"  Cohen's Kappa: {kappa:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# ===================== EVALUATE ALL MODELS =====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATING ALL MODELS ON TEST SET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "evaluation_results = {}\n",
        "\n",
        "for model_name in training_results.keys():\n",
        "    model = training_results[model_name]['model']\n",
        "    results = evaluate_model(model, model_name, X_test, y_test)\n",
        "    evaluation_results[model_name] = results\n",
        "\n",
        "# ===================== CONFUSION MATRIX PLOTTING =====================\n",
        "def plot_confusion_matrix(model_name, cm, classes):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Normalize confusion matrix\n",
        "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                xticklabels=classes, yticklabels=classes,\n",
        "                cbar_kws={'label': 'Normalized Count'})\n",
        "\n",
        "    plt.title(f'{model_name} - Confusion Matrix\\nAccuracy: {evaluation_results[model_name][\"accuracy\"]:.4f}',\n",
        "              fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('True Label', fontsize=12)\n",
        "    plt.xlabel('Predicted Label', fontsize=12)\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{model_name}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(f\"✓ Confusion matrix saved: {model_name}_confusion_matrix.png\")\n",
        "\n",
        "# Plot confusion matrices for all models\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GENERATING CONFUSION MATRICES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for model_name in evaluation_results.keys():\n",
        "    cm = evaluation_results[model_name]['confusion_matrix']\n",
        "    plot_confusion_matrix(model_name, cm, label_encoder.classes_)\n",
        "\n",
        "# ===================== DETAILED CLASSIFICATION REPORTS =====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED CLASSIFICATION REPORTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for model_name in evaluation_results.keys():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{model_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    report = evaluation_results[model_name]['report']\n",
        "\n",
        "    # Print per-class metrics\n",
        "    print(f\"\\n{'Emotion':<15} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for emotion in label_encoder.classes_:\n",
        "        metrics = report[emotion]\n",
        "        print(f\"{emotion:<15} {metrics['precision']:<12.4f} {metrics['recall']:<12.4f} \"\n",
        "              f\"{metrics['f1-score']:<12.4f} {int(metrics['support']):<10}\")\n",
        "\n",
        "    # Print overall metrics\n",
        "    print(\"-\" * 80)\n",
        "    print(f\"\\nOverall Metrics:\")\n",
        "    print(f\"  Accuracy:           {evaluation_results[model_name]['accuracy']:.4f}\")\n",
        "    print(f\"  Macro Avg F1:       {evaluation_results[model_name]['f1_macro']:.4f}\")\n",
        "    print(f\"  Weighted Avg F1:    {evaluation_results[model_name]['f1_weighted']:.4f}\")\n",
        "    print(f\"  Cohen's Kappa:      {evaluation_results[model_name]['kappa']:.4f}\")\n",
        "\n",
        "# ===================== MODEL COMPARISON =====================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_data = []\n",
        "for model_name in evaluation_results.keys():\n",
        "    comparison_data.append({\n",
        "        'Model': model_name,\n",
        "        'Accuracy': evaluation_results[model_name]['accuracy'],\n",
        "        'F1-Macro': evaluation_results[model_name]['f1_macro'],\n",
        "        'F1-Weighted': evaluation_results[model_name]['f1_weighted'],\n",
        "        'Precision': evaluation_results[model_name]['precision_weighted'],\n",
        "        'Recall': evaluation_results[model_name]['recall_weighted'],\n",
        "        'Kappa': evaluation_results[model_name]['kappa']\n",
        "    })\n",
        "\n",
        "df_comparison = pd.DataFrame(comparison_data)\n",
        "df_comparison = df_comparison.sort_values('Accuracy', ascending=False)\n",
        "\n",
        "print(\"\\n\" + df_comparison.to_string(index=False))\n",
        "print()\n",
        "\n",
        "# Save comparison table\n",
        "df_comparison.to_csv('model_comparison.csv', index=False)\n",
        "print(\"✓ Comparison table saved: model_comparison.csv\")\n",
        "\n",
        "# ===================== COMPARISON VISUALIZATION =====================\n",
        "def plot_model_comparison(df):\n",
        "    \"\"\"Plot comprehensive model comparison\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "\n",
        "    metrics = ['Accuracy', 'F1-Macro', 'F1-Weighted', 'Kappa']\n",
        "    colors = ['steelblue', 'coral', 'seagreen', 'mediumpurple']\n",
        "\n",
        "    for idx, (metric, color) in enumerate(zip(metrics, colors)):\n",
        "        ax = axes[idx // 2, idx % 2]\n",
        "\n",
        "        df_sorted = df.sort_values(metric, ascending=True)\n",
        "        bars = ax.barh(df_sorted['Model'], df_sorted[metric], color=color, alpha=0.8)\n",
        "\n",
        "        # Add value labels\n",
        "        for bar in bars:\n",
        "            width = bar.get_width()\n",
        "            ax.text(width, bar.get_y() + bar.get_height()/2,\n",
        "                   f'{width:.4f}',\n",
        "                   ha='left', va='center', fontsize=10, fontweight='bold')\n",
        "\n",
        "        ax.set_xlabel(metric, fontsize=12, fontweight='bold')\n",
        "        ax.set_title(f'Model Comparison - {metric}', fontsize=14, fontweight='bold')\n",
        "        ax.set_xlim(0, 1)\n",
        "        ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_comparison_chart.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"✓ Comparison chart saved: model_comparison_chart.png\")\n",
        "\n",
        "plot_model_comparison(df_comparison)\n",
        "\n",
        "# ===================== FIND BEST MODEL =====================\n",
        "best_model_name = df_comparison.iloc[0]['Model']\n",
        "best_accuracy = df_comparison.iloc[0]['Accuracy']\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BEST MODEL\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
        "print(f\"   Accuracy: {best_accuracy:.4f}\")\n",
        "print(f\"   F1-Score (Macro): {df_comparison.iloc[0]['F1-Macro']:.4f}\")\n",
        "print(f\"   F1-Score (Weighted): {df_comparison.iloc[0]['F1-Weighted']:.4f}\")\n",
        "print(f\"   Cohen's Kappa: {df_comparison.iloc[0]['Kappa']:.4f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# ===================== PER-CLASS PERFORMANCE COMPARISON =====================\n",
        "def plot_per_class_comparison():\n",
        "    \"\"\"Compare all models' performance per emotion class\"\"\"\n",
        "    emotions = label_encoder.classes_\n",
        "\n",
        "    # Collect F1-scores for each emotion from each model\n",
        "    data_for_plot = []\n",
        "    for model_name in evaluation_results.keys():\n",
        "        report = evaluation_results[model_name]['report']\n",
        "        for emotion in emotions:\n",
        "            data_for_plot.append({\n",
        "                'Model': model_name,\n",
        "                'Emotion': emotion,\n",
        "                'F1-Score': report[emotion]['f1-score']\n",
        "            })\n",
        "\n",
        "    df_per_class = pd.DataFrame(data_for_plot)\n",
        "\n",
        "    # Create grouped bar chart\n",
        "    fig, ax = plt.subplots(figsize=(16, 8))\n",
        "\n",
        "    emotions_list = list(emotions)\n",
        "    x = np.arange(len(emotions_list))\n",
        "    width = 0.1\n",
        "\n",
        "    model_names = list(evaluation_results.keys())\n",
        "    colors_palette = plt.cm.Set3(np.linspace(0, 1, len(model_names)))\n",
        "\n",
        "    for idx, model_name in enumerate(model_names):\n",
        "        model_data = df_per_class[df_per_class['Model'] == model_name]\n",
        "        f1_scores = [model_data[model_data['Emotion'] == e]['F1-Score'].values[0]\n",
        "                    for e in emotions_list]\n",
        "        offset = width * (idx - len(model_names)/2)\n",
        "        ax.bar(x + offset, f1_scores, width, label=model_name,\n",
        "               color=colors_palette[idx], alpha=0.8)\n",
        "\n",
        "    ax.set_xlabel('Emotion', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('F1-Score', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('Per-Class Performance Comparison Across All Models',\n",
        "                fontsize=14, fontweight='bold')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(emotions_list, rotation=45, ha='right')\n",
        "    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    ax.set_ylim(0, 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('per_class_comparison.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print(\"✓ Per-class comparison saved: per_class_comparison.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PER-CLASS PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "plot_per_class_comparison()\n",
        "\n",
        "# ===================== SAVE FINAL RESULTS =====================\n",
        "with open('evaluation_results.pkl', 'wb') as f:\n",
        "    pickle.dump(evaluation_results, f)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"✅ EVALUATION COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nAll results saved:\")\n",
        "print(\"  - evaluation_results.pkl\")\n",
        "print(\"  - model_comparison.csv\")\n",
        "print(\"  - Confusion matrices (PNG)\")\n",
        "print(\"  - Comparison charts (PNG)\")\n",
        "print(\"\\nBest model:\", best_model_name)\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "GUZH8xcT9mGU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}